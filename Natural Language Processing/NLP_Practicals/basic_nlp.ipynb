{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                NLP can also help you write better content by providing feedback, suggestions, \n",
      "                and insights on your writing style, tone, readability, grammar, and SEO. \n",
      "                You can use NLP tools to check and improve your spelling, punctuation, vocabulary, \n",
      "                and sentence structure. You can also use NLP tools to measure and adjust your \n",
      "                content's sentiment, emotion, and personality to match your brand voice and your \n",
      "                audience's preferences. For example, you can use a tool like Hemingway to make your \n",
      "                content more clear and concise, or use a tool like Grammarly to enhance your writing \n",
      "                skills and avoid errors.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "paragraph = '''\n",
    "                NLP can also help you write better content by providing feedback, suggestions, \n",
    "                and insights on your writing style, tone, readability, grammar, and SEO. \n",
    "                You can use NLP tools to check and improve your spelling, punctuation, vocabulary, \n",
    "                and sentence structure. You can also use NLP tools to measure and adjust your \n",
    "                content's sentiment, emotion, and personality to match your brand voice and your \n",
    "                audience's preferences. For example, you can use a tool like Hemingway to make your \n",
    "                content more clear and concise, or use a tool like Grammarly to enhance your writing \n",
    "                skills and avoid errors.\n",
    "            '''\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "Convert paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "nltk.download('all')\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n                NLP can also help you write better content by providing feedback, suggestions, \\n                and insights on your writing style, tone, readability, grammar, and SEO.', 'You can use NLP tools to check and improve your spelling, punctuation, vocabulary, \\n                and sentence structure.', \"You can also use NLP tools to measure and adjust your \\n                content's sentiment, emotion, and personality to match your brand voice and your \\n                audience's preferences.\", 'For example, you can use a tool like Hemingway to make your \\n                content more clear and concise, or use a tool like Grammarly to enhance your writing \\n                skills and avoid errors.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "goe\n",
      "fairli\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('history'))\n",
    "print(stemmer.stem('goes'))\n",
    "print(stemmer.stem('fairly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Regexpstemming***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stemming class will use regex which will first apply on the word and convert the word into it's stem word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "go\n",
      "history\n",
      "fairly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex_stem = RegexpStemmer('ing$|s$|es$|able$', min=4)\n",
    "print(regex_stem.stem('eating'))\n",
    "print(regex_stem.stem('goes'))\n",
    "print(regex_stem.stem(\"history\"))\n",
    "print(regex_stem.stem(\"fairly\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Snowball Stemmer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the better form of word compare to above two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "goe\n",
      "histori\n",
      "fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stem = SnowballStemmer('english')\n",
    "\n",
    "print(snowball_stem.stem('eating'))\n",
    "print(snowball_stem.stem('goes'))\n",
    "print(snowball_stem.stem(\"history\"))\n",
    "print(snowball_stem.stem(\"fairly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lemmetization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmetizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating\n",
      "go\n",
      "history\n",
      "fairly\n"
     ]
    }
   ],
   "source": [
    "print(lemmetizer.lemmatize('eating',pos='n'))  # pos is used for part of speech like n = Noun, v = verb, a = adjective, r = adverb\n",
    "print(lemmetizer.lemmatize('goes', pos='v'))\n",
    "print(lemmetizer.lemmatize(\"history\"))\n",
    "print(lemmetizer.lemmatize(\"fairly\", pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text-preprocessing for the paragarph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing all special charecters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                NLP can also help you write better content by providing feedback, suggestions, \n",
      "                and insights on your writing style, tone, readability, grammar, and SEO.\n",
      "You can use NLP tools to check and improve your spelling, punctuation, vocabulary, \n",
      "                and sentence structure.\n",
      "You can also use NLP tools to measure and adjust your \n",
      "                content's sentiment, emotion, and personality to match your brand voice and your \n",
      "                audience's preferences.\n",
      "For example, you can use a tool like Hemingway to make your \n",
      "                content more clear and concise, or use a tool like Grammarly to enhance your writing \n",
      "                skills and avoid errors.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]).lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                 nlp can also help you write better content by providing feedback  suggestions                   and insights on your writing style  tone  readability  grammar  and seo ',\n",
       " 'you can use nlp tools to check and improve your spelling  punctuation  vocabulary                   and sentence structure ',\n",
       " 'you can also use nlp tools to measure and adjust your                  content s sentiment  emotion  and personality to match your brand voice and your                  audience s preferences ',\n",
       " 'for example  you can use a tool like hemingway to make your                  content more clear and concise  or use a tool like grammarly to enhance your writing                  skills and avoid errors ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming & Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem word for nlp is nlp\n",
      "lemmatizer word for nlp is nlp\n",
      "stem word for also is also\n",
      "lemmatizer word for also is also\n",
      "stem word for help is help\n",
      "lemmatizer word for help is help\n",
      "stem word for write is write\n",
      "lemmatizer word for write is write\n",
      "stem word for better is better\n",
      "lemmatizer word for better is better\n",
      "stem word for content is content\n",
      "lemmatizer word for content is content\n",
      "stem word for providing is provid\n",
      "lemmatizer word for providing is providing\n",
      "stem word for feedback is feedback\n",
      "lemmatizer word for feedback is feedback\n",
      "stem word for suggestions is suggest\n",
      "lemmatizer word for suggestions is suggestion\n",
      "stem word for insights is insight\n",
      "lemmatizer word for insights is insight\n",
      "stem word for writing is write\n",
      "lemmatizer word for writing is writing\n",
      "stem word for style is style\n",
      "lemmatizer word for style is style\n",
      "stem word for tone is tone\n",
      "lemmatizer word for tone is tone\n",
      "stem word for readability is readabl\n",
      "lemmatizer word for readability is readability\n",
      "stem word for grammar is grammar\n",
      "lemmatizer word for grammar is grammar\n",
      "stem word for seo is seo\n",
      "lemmatizer word for seo is seo\n",
      "stem word for use is use\n",
      "lemmatizer word for use is use\n",
      "stem word for nlp is nlp\n",
      "lemmatizer word for nlp is nlp\n",
      "stem word for tools is tool\n",
      "lemmatizer word for tools is tool\n",
      "stem word for check is check\n",
      "lemmatizer word for check is check\n",
      "stem word for improve is improv\n",
      "lemmatizer word for improve is improve\n",
      "stem word for spelling is spell\n",
      "lemmatizer word for spelling is spelling\n",
      "stem word for punctuation is punctuat\n",
      "lemmatizer word for punctuation is punctuation\n",
      "stem word for vocabulary is vocabulari\n",
      "lemmatizer word for vocabulary is vocabulary\n",
      "stem word for sentence is sentenc\n",
      "lemmatizer word for sentence is sentence\n",
      "stem word for structure is structur\n",
      "lemmatizer word for structure is structure\n",
      "stem word for also is also\n",
      "lemmatizer word for also is also\n",
      "stem word for use is use\n",
      "lemmatizer word for use is use\n",
      "stem word for nlp is nlp\n",
      "lemmatizer word for nlp is nlp\n",
      "stem word for tools is tool\n",
      "lemmatizer word for tools is tool\n",
      "stem word for measure is measur\n",
      "lemmatizer word for measure is measure\n",
      "stem word for adjust is adjust\n",
      "lemmatizer word for adjust is adjust\n",
      "stem word for content is content\n",
      "lemmatizer word for content is content\n",
      "stem word for sentiment is sentiment\n",
      "lemmatizer word for sentiment is sentiment\n",
      "stem word for emotion is emot\n",
      "lemmatizer word for emotion is emotion\n",
      "stem word for personality is person\n",
      "lemmatizer word for personality is personality\n",
      "stem word for match is match\n",
      "lemmatizer word for match is match\n",
      "stem word for brand is brand\n",
      "lemmatizer word for brand is brand\n",
      "stem word for voice is voic\n",
      "lemmatizer word for voice is voice\n",
      "stem word for audience is audienc\n",
      "lemmatizer word for audience is audience\n",
      "stem word for preferences is prefer\n",
      "lemmatizer word for preferences is preference\n",
      "stem word for example is exampl\n",
      "lemmatizer word for example is example\n",
      "stem word for use is use\n",
      "lemmatizer word for use is use\n",
      "stem word for tool is tool\n",
      "lemmatizer word for tool is tool\n",
      "stem word for like is like\n",
      "lemmatizer word for like is like\n",
      "stem word for hemingway is hemingway\n",
      "lemmatizer word for hemingway is hemingway\n",
      "stem word for make is make\n",
      "lemmatizer word for make is make\n",
      "stem word for content is content\n",
      "lemmatizer word for content is content\n",
      "stem word for clear is clear\n",
      "lemmatizer word for clear is clear\n",
      "stem word for concise is concis\n",
      "lemmatizer word for concise is concise\n",
      "stem word for use is use\n",
      "lemmatizer word for use is use\n",
      "stem word for tool is tool\n",
      "lemmatizer word for tool is tool\n",
      "stem word for like is like\n",
      "lemmatizer word for like is like\n",
      "stem word for grammarly is grammarli\n",
      "lemmatizer word for grammarly is grammarly\n",
      "stem word for enhance is enhanc\n",
      "lemmatizer word for enhance is enhance\n",
      "stem word for writing is write\n",
      "lemmatizer word for writing is writing\n",
      "stem word for skills is skill\n",
      "lemmatizer word for skills is skill\n",
      "stem word for avoid is avoid\n",
      "lemmatizer word for avoid is avoid\n",
      "stem word for errors is error\n",
      "lemmatizer word for errors is error\n"
     ]
    }
   ],
   "source": [
    "for sentence in corpus:  # Iterate through each sentence in corpus\n",
    "    words = nltk.word_tokenize(sentence) # Convert sentence into words using word_tokenize\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):  # If word is not present in set of english stopwrds\n",
    "            print(f\"stem word for {word} is {stemmer.stem(word)}\")\n",
    "            print(f\"lemmatizer word for {word} is {lemmetizer.lemmatize(word)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parts of Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nlp', 'NN'), ('can', 'MD'), ('also', 'RB'), ('help', 'VB'), ('you', 'PRP'), ('write', 'VB'), ('better', 'RBR'), ('content', 'NN'), ('by', 'IN'), ('providing', 'VBG'), ('feedback', 'JJ'), ('suggestions', 'NNS'), ('and', 'CC'), ('insights', 'NNS'), ('on', 'IN'), ('your', 'PRP$'), ('writing', 'VBG'), ('style', 'NN'), ('tone', 'NN'), ('readability', 'NN'), ('grammar', 'NN'), ('and', 'CC'), ('seo', 'NN')]\n",
      "[('you', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('nlp', 'JJ'), ('tools', 'NNS'), ('to', 'TO'), ('check', 'VB'), ('and', 'CC'), ('improve', 'VB'), ('your', 'PRP$'), ('spelling', 'VBG'), ('punctuation', 'NN'), ('vocabulary', 'NN'), ('and', 'CC'), ('sentence', 'NN'), ('structure', 'NN')]\n",
      "[('you', 'PRP'), ('can', 'MD'), ('also', 'RB'), ('use', 'VB'), ('nlp', 'JJ'), ('tools', 'NNS'), ('to', 'TO'), ('measure', 'VB'), ('and', 'CC'), ('adjust', 'VB'), ('your', 'PRP$'), ('content', 'NN'), ('s', 'JJ'), ('sentiment', 'NN'), ('emotion', 'NN'), ('and', 'CC'), ('personality', 'NN'), ('to', 'TO'), ('match', 'VB'), ('your', 'PRP$'), ('brand', 'NN'), ('voice', 'NN'), ('and', 'CC'), ('your', 'PRP$'), ('audience', 'NN'), ('s', 'NN'), ('preferences', 'NNS')]\n",
      "[('for', 'IN'), ('example', 'NN'), ('you', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('a', 'DT'), ('tool', 'NN'), ('like', 'IN'), ('hemingway', 'NN'), ('to', 'TO'), ('make', 'VB'), ('your', 'PRP$'), ('content', 'NN'), ('more', 'JJR'), ('clear', 'JJ'), ('and', 'CC'), ('concise', 'NN'), ('or', 'CC'), ('use', 'VB'), ('a', 'DT'), ('tool', 'NN'), ('like', 'IN'), ('grammarly', 'RB'), ('to', 'TO'), ('enhance', 'VB'), ('your', 'PRP$'), ('writing', 'VBG'), ('skills', 'NNS'), ('and', 'CC'), ('avoid', 'JJ'), ('errors', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in corpus:\n",
    "    tagged_words = nltk.pos_tag(sentence.split())\n",
    "    print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset('RB'))\n",
    "print(nltk.help.upenn_tagset('MD'))\n",
    "print(nltk.help.upenn_tagset('PRP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NER (Named Entity Recognition)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will find out the entity out of the words also with the part of speech of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tagged_words).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words (BOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorize = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['adjust', 'also', 'and', 'audience', 'avoid', 'better', 'brand',\n",
       "       'by', 'can', 'check', 'clear', 'concise', 'content', 'emotion',\n",
       "       'enhance', 'errors', 'example', 'feedback', 'for', 'grammar',\n",
       "       'grammarly', 'help', 'hemingway', 'improve', 'insights', 'like',\n",
       "       'make', 'match', 'measure', 'more', 'nlp', 'on', 'or',\n",
       "       'personality', 'preferences', 'providing', 'punctuation',\n",
       "       'readability', 'sentence', 'sentiment', 'seo', 'skills',\n",
       "       'spelling', 'structure', 'style', 'suggestions', 'to', 'tone',\n",
       "       'tool', 'tools', 'use', 'vocabulary', 'voice', 'write', 'writing',\n",
       "       'you', 'your'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = count_vectorize.fit_transform(corpus)\n",
    "count_vectorize.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp': 30,\n",
       " 'can': 8,\n",
       " 'also': 1,\n",
       " 'help': 21,\n",
       " 'you': 55,\n",
       " 'write': 53,\n",
       " 'better': 5,\n",
       " 'content': 12,\n",
       " 'by': 7,\n",
       " 'providing': 35,\n",
       " 'feedback': 17,\n",
       " 'suggestions': 45,\n",
       " 'and': 2,\n",
       " 'insights': 24,\n",
       " 'on': 31,\n",
       " 'your': 56,\n",
       " 'writing': 54,\n",
       " 'style': 44,\n",
       " 'tone': 47,\n",
       " 'readability': 37,\n",
       " 'grammar': 19,\n",
       " 'seo': 40,\n",
       " 'use': 50,\n",
       " 'tools': 49,\n",
       " 'to': 46,\n",
       " 'check': 9,\n",
       " 'improve': 23,\n",
       " 'spelling': 42,\n",
       " 'punctuation': 36,\n",
       " 'vocabulary': 51,\n",
       " 'sentence': 38,\n",
       " 'structure': 43,\n",
       " 'measure': 28,\n",
       " 'adjust': 0,\n",
       " 'sentiment': 39,\n",
       " 'emotion': 13,\n",
       " 'personality': 33,\n",
       " 'match': 27,\n",
       " 'brand': 6,\n",
       " 'voice': 52,\n",
       " 'audience': 3,\n",
       " 'preferences': 34,\n",
       " 'for': 18,\n",
       " 'example': 16,\n",
       " 'tool': 48,\n",
       " 'like': 25,\n",
       " 'hemingway': 22,\n",
       " 'make': 26,\n",
       " 'more': 29,\n",
       " 'clear': 10,\n",
       " 'concise': 11,\n",
       " 'or': 32,\n",
       " 'grammarly': 20,\n",
       " 'enhance': 14,\n",
       " 'skills': 41,\n",
       " 'avoid': 4,\n",
       " 'errors': 15}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorize.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What does the CountVectorizer does?**\n",
    "\n",
    "- So CountVectorizer will create a sparse matrix of vocabulary from the corpus\n",
    "- get_feature_names_out() this method gives us the list of words in vocabulary.\n",
    "- vocabulary_ gives us the word index in vocabulary\n",
    "- If we want binary BOW then we can set binary to True\n",
    "- Also we can use ngrams to create a pair of words if we use (3,3) then it will create trigrams.\n",
    "- If we use (2, 3) then it will first create bigrams and after that it will create trigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 nlp can also help you write better content by providing feedback  suggestions                   and insights on your writing style  tone  readability  grammar  and seo '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp can': 83,\n",
       " 'can also': 31,\n",
       " 'also help': 2,\n",
       " 'help you': 63,\n",
       " 'you write': 145,\n",
       " 'write better': 136,\n",
       " 'better content': 25,\n",
       " 'content by': 43,\n",
       " 'by providing': 29,\n",
       " 'providing feedback': 93,\n",
       " 'feedback suggestions': 55,\n",
       " 'suggestions and': 108,\n",
       " 'and insights': 14,\n",
       " 'insights on': 69,\n",
       " 'on your': 87,\n",
       " 'your writing': 156,\n",
       " 'writing style': 140,\n",
       " 'style tone': 106,\n",
       " 'tone readability': 120,\n",
       " 'readability grammar': 97,\n",
       " 'grammar and': 59,\n",
       " 'and seo': 20,\n",
       " 'nlp can also': 84,\n",
       " 'can also help': 32,\n",
       " 'also help you': 3,\n",
       " 'help you write': 64,\n",
       " 'you write better': 146,\n",
       " 'write better content': 137,\n",
       " 'better content by': 26,\n",
       " 'content by providing': 44,\n",
       " 'by providing feedback': 30,\n",
       " 'providing feedback suggestions': 94,\n",
       " 'feedback suggestions and': 56,\n",
       " 'suggestions and insights': 109,\n",
       " 'and insights on': 15,\n",
       " 'insights on your': 70,\n",
       " 'on your writing': 88,\n",
       " 'your writing style': 158,\n",
       " 'writing style tone': 141,\n",
       " 'style tone readability': 107,\n",
       " 'tone readability grammar': 121,\n",
       " 'readability grammar and': 98,\n",
       " 'grammar and seo': 60,\n",
       " 'you can': 142,\n",
       " 'can use': 34,\n",
       " 'use nlp': 128,\n",
       " 'nlp tools': 85,\n",
       " 'tools to': 125,\n",
       " 'to check': 110,\n",
       " 'check and': 37,\n",
       " 'and improve': 12,\n",
       " 'improve your': 67,\n",
       " 'your spelling': 154,\n",
       " 'spelling punctuation': 104,\n",
       " 'punctuation vocabulary': 95,\n",
       " 'vocabulary and': 132,\n",
       " 'and sentence': 18,\n",
       " 'sentence structure': 99,\n",
       " 'you can use': 144,\n",
       " 'can use nlp': 35,\n",
       " 'use nlp tools': 129,\n",
       " 'nlp tools to': 86,\n",
       " 'tools to check': 126,\n",
       " 'to check and': 111,\n",
       " 'check and improve': 38,\n",
       " 'and improve your': 13,\n",
       " 'improve your spelling': 68,\n",
       " 'your spelling punctuation': 155,\n",
       " 'spelling punctuation vocabulary': 105,\n",
       " 'punctuation vocabulary and': 96,\n",
       " 'vocabulary and sentence': 133,\n",
       " 'and sentence structure': 19,\n",
       " 'also use': 4,\n",
       " 'to measure': 118,\n",
       " 'measure and': 79,\n",
       " 'and adjust': 6,\n",
       " 'adjust your': 0,\n",
       " 'your content': 151,\n",
       " 'content sentiment': 47,\n",
       " 'sentiment emotion': 100,\n",
       " 'emotion and': 49,\n",
       " 'and personality': 16,\n",
       " 'personality to': 91,\n",
       " 'to match': 116,\n",
       " 'match your': 77,\n",
       " 'your brand': 149,\n",
       " 'brand voice': 27,\n",
       " 'voice and': 134,\n",
       " 'and your': 21,\n",
       " 'your audience': 147,\n",
       " 'audience preferences': 23,\n",
       " 'you can also': 143,\n",
       " 'can also use': 33,\n",
       " 'also use nlp': 5,\n",
       " 'tools to measure': 127,\n",
       " 'to measure and': 119,\n",
       " 'measure and adjust': 80,\n",
       " 'and adjust your': 7,\n",
       " 'adjust your content': 1,\n",
       " 'your content sentiment': 153,\n",
       " 'content sentiment emotion': 48,\n",
       " 'sentiment emotion and': 101,\n",
       " 'emotion and personality': 50,\n",
       " 'and personality to': 17,\n",
       " 'personality to match': 92,\n",
       " 'to match your': 117,\n",
       " 'match your brand': 78,\n",
       " 'your brand voice': 150,\n",
       " 'brand voice and': 28,\n",
       " 'voice and your': 135,\n",
       " 'and your audience': 22,\n",
       " 'your audience preferences': 148,\n",
       " 'for example': 57,\n",
       " 'example you': 53,\n",
       " 'use tool': 130,\n",
       " 'tool like': 122,\n",
       " 'like hemingway': 73,\n",
       " 'hemingway to': 65,\n",
       " 'to make': 114,\n",
       " 'make your': 75,\n",
       " 'content more': 45,\n",
       " 'more clear': 81,\n",
       " 'clear and': 39,\n",
       " 'and concise': 10,\n",
       " 'concise or': 41,\n",
       " 'or use': 89,\n",
       " 'like grammarly': 71,\n",
       " 'grammarly to': 61,\n",
       " 'to enhance': 112,\n",
       " 'enhance your': 51,\n",
       " 'writing skills': 138,\n",
       " 'skills and': 102,\n",
       " 'and avoid': 8,\n",
       " 'avoid errors': 24,\n",
       " 'for example you': 58,\n",
       " 'example you can': 54,\n",
       " 'can use tool': 36,\n",
       " 'use tool like': 131,\n",
       " 'tool like hemingway': 124,\n",
       " 'like hemingway to': 74,\n",
       " 'hemingway to make': 66,\n",
       " 'to make your': 115,\n",
       " 'make your content': 76,\n",
       " 'your content more': 152,\n",
       " 'content more clear': 46,\n",
       " 'more clear and': 82,\n",
       " 'clear and concise': 40,\n",
       " 'and concise or': 11,\n",
       " 'concise or use': 42,\n",
       " 'or use tool': 90,\n",
       " 'tool like grammarly': 123,\n",
       " 'like grammarly to': 72,\n",
       " 'grammarly to enhance': 62,\n",
       " 'to enhance your': 113,\n",
       " 'enhance your writing': 52,\n",
       " 'your writing skills': 157,\n",
       " 'writing skills and': 139,\n",
       " 'skills and avoid': 103,\n",
       " 'and avoid errors': 9}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorize = CountVectorizer(max_features = 500, binary=True, ngram_range=(2, 3))\n",
    "\n",
    "X = count_vectorize.fit_transform(corpus).toarray()\n",
    "count_vectorize.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will give us a clear idea that Bag Of Words for 1st sentence has second element from vocabulary which is also. so here we are given 1.\n",
    "- and is present 2 times hence we have given 2.\n",
    "- If we use Binary = True then we will get only 0 & 1 even if the word is present more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 nlp can also help you write better content by providing feedback  suggestions                   and insights on your writing style  tone  readability  grammar  and seo '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.18601526, 0.24624319, 0.        , 0.        ,\n",
       "        0.23593677, 0.        , 0.23593677, 0.12312159, 0.        ,\n",
       "        0.        , 0.        , 0.15059538, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23593677, 0.        , 0.23593677,\n",
       "        0.        , 0.23593677, 0.        , 0.        , 0.23593677,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.15059538, 0.23593677, 0.        , 0.        , 0.        ,\n",
       "        0.23593677, 0.        , 0.23593677, 0.        , 0.        ,\n",
       "        0.23593677, 0.        , 0.        , 0.        , 0.23593677,\n",
       "        0.23593677, 0.        , 0.23593677, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.23593677, 0.18601526,\n",
       "        0.12312159, 0.12312159]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So here we are getting different values to different words instead of just 1 & 0.\n",
    "- So in this way we are capturing the important words from the sentences.\n",
    "- Here also we can use ngrams, to pair up the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
